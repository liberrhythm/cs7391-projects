{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swlist = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "corpus1 = brown\n",
    "corpus2 = state_union\n",
    "\n",
    "docs = corpus1.fileids()\n",
    "num_docs = len(corpus1.fileids())\n",
    "print(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, brown, state_union, shakespeare, stopwords\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import *\n",
    "import string, math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD PREPROCESS\n",
    "\n",
    "def preprocess(words):\n",
    "    stemmer = self.stemmer\n",
    "    swlist = self.swlist\n",
    "    \n",
    "    # ignore case / take care of punctuation\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)).lower() for word in words\n",
    "                if len(word) > 2]\n",
    "    \n",
    "    # stem words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # check for stopwords and remove any empty strings\n",
    "    words = [word for word in words if word and not word in swlist]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW PREPROCESS\n",
    "\n",
    "def preprocess(self, words):\n",
    "    stemmer = self.stemmer\n",
    "    swlist = self.swlist\n",
    "    \n",
    "    # ignore case / take care of punctuation\n",
    "    if (self.ignorecase == 'no'):\n",
    "        words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words\n",
    "                if len(word) > 2]\n",
    "    else:\n",
    "        words = [word.translate(str.maketrans('', '', string.punctuation)).lower() for word in words\n",
    "                if len(word) > 2]\n",
    "    \n",
    "    # stem words\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # check for stopwords and remove any empty strings\n",
    "    words = [word for word in words if word and not word in swlist]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, corpus, tf='raw', idf='base', stopword='none', stemmer=PorterStemmer(), ignorecase='yes'):\n",
    "    # set parameters\n",
    "    self.corpus = corpus\n",
    "    self.tf = tf\n",
    "    self.idf = idf\n",
    "    self.stopword = stopword # which one to use?\n",
    "    self.stemmer = stemmer # which one to use?\n",
    "    self.ignorecase = ignorecase\n",
    "    \n",
    "    # helper funcs\n",
    "    self.swlist = get_stopwords()\n",
    "    self.docs = corpus.fileids()\n",
    "    self.all_words = get_all_words()\n",
    "    self.unique_words = get_unique_words()\n",
    "    self.tf_vectors = get_tf()\n",
    "    self.idf_vec = get_idf()\n",
    "    self.tfidf_vectors = get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Things to do:\n",
    "# stemmer\n",
    "# ignorecase\n",
    "# copy descriptions of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_stopwords(stopword):\n",
    "#     if (stopword == 'none'):\n",
    "#         return set()\n",
    "#     elif (stopword == ''):\n",
    "#         return set(stopwords.words('english'))\n",
    "#     else:\n",
    "#         with open(stopword) as f:\n",
    "#             stopword_list = [word for line in f for word in re.split('; |, |\\*|\\n', line) if word]\n",
    "        \n",
    "#         f.close()\n",
    "#         return set(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword = 'stopwords.txt'\n",
    "# sw = get_stopwords(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(self):\n",
    "    if (self.stopword == 'none'):\n",
    "        return set()\n",
    "    elif (self.stopword == ''):\n",
    "        return set(stopwords.words('english'))\n",
    "    else:\n",
    "        with open(self.stopword) as f:\n",
    "            stopword_list = [word for line in f for word in re.split('; |, |\\*|\\n', line) if word]\n",
    "        \n",
    "        f.close()\n",
    "        return set(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(self):\n",
    "    all_words = preprocess(self.corpus.words())\n",
    "    all_words = np.array(all_words)\n",
    "    return np.sort(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(self):\n",
    "    return np.unique(self.all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(self):\n",
    "    tf_vectors = np.zeros(shape=(len(self.docs), len(self.unique_words)))\n",
    "    \n",
    "    doc_idx = 0\n",
    "    for doc in self.docs:\n",
    "        doc_words = self.corpus.words(fileids=[doc])\n",
    "        doc_words = preprocess(doc_words)\n",
    "        doc_freq = FreqDist(doc_words)\n",
    "    \n",
    "        word_idx = 0\n",
    "        for word in unique_words:\n",
    "            if word in doc_freq:\n",
    "                tf_vectors[doc_idx][word_idx] = doc_freq[word]\n",
    "            word_idx += 1\n",
    "        \n",
    "        doc_idx += 1\n",
    "        \n",
    "    if (self.tf == 'log'):\n",
    "        tf_vectors_log = np.log2(tf_vectors, where=(tf_vectors > 0))\n",
    "        tf_vectors_log[tf_vectors > 0] += 1\n",
    "        return tf_vectors_log\n",
    "    elif (self.tf == 'binary'):\n",
    "        tf_vectors_binary = (tf_vectors > 0).astype(int)\n",
    "        return tf_vectors_binary\n",
    "    else:\n",
    "        return tf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(self):\n",
    "    idf_vec = np.zeros(shape=len(self.unique_words))\n",
    "    \n",
    "    counter = 0\n",
    "    for word_vec in self.tf_vectors.T:\n",
    "        idf_vec[counter] = np.count_nonzero(word_vec)\n",
    "        counter += 1\n",
    "        \n",
    "    if (self.idf == 'smooth'):\n",
    "        idf_vec = num_docs / idf_vec\n",
    "        idf_vec_smooth = np.log2(idf_vec + 1)\n",
    "        return idf_vec_smooth \n",
    "    elif (self.idf == 'probabilistic'):\n",
    "        idf_vec = (idf_vec - num_docs) / idf_vec\n",
    "        idf_vec_prob = np.log2(idf_vec)\n",
    "        idf_vec_prob[idf_vec_prob < 0] = 0\n",
    "        return idf_vec_prob\n",
    "    else:\n",
    "        idf_vec = num_docs / idf_vec\n",
    "        idf_vec = np.log2(idf_vec)\n",
    "        return idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.array([1,0.05,0.2,2,0.6,3])\n",
    "# a = np.log2(arr)\n",
    "# print(a)\n",
    "# a[a < 0] = 0\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(self):\n",
    "    return self.tf_vectors * self.idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self):\n",
    "    return self.tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self, fileid):\n",
    "    doc_idx[0][0] = np.where(docs == fileid)\n",
    "    return self.tfidf_vectors[doc_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(self, filelist):\n",
    "    filemask = np.isin(docs, filelist)\n",
    "    indices = np.where(filemask)[0]\n",
    "    return self.tfidf_vectors(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_dim(self):\n",
    "    return self.unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_new(self, words):\n",
    "    tf_vec = np.zeros(shape=(, len(self.unique_words)))\n",
    "    doc_freq = FreqDist(preprocess(words, self.swlist))\n",
    "    \n",
    "    for word in self.unique_words:\n",
    "        if word in doc_freq:\n",
    "            tf_vec[word_idx] = doc_freq[word]\n",
    "        word_idx += 1\n",
    "        \n",
    "    return vec * self.idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(self, fileid):\n",
    "    filemask = np.isin(docs, fileid)\n",
    "    indices = np.where(filemask)[0]\n",
    "    vec1 = self.tfidf_vectors[indices[0],]\n",
    "    vec2 = self.tfidf_vectors[indices[1],]\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim_new(self, words, fileid):\n",
    "    vec1 = tf_idf_new(words)\n",
    "    vec2 = tf_idf(fileid)\n",
    "    numerator = np.dot(vec1, vec2)\n",
    "    denominator = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
