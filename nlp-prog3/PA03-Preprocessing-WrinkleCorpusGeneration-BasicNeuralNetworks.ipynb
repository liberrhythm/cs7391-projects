{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3: Open-Ended ML Approach\n",
    "Sabrina Peng, CS7391 - Spring 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import math, statistics, random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "swlist = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "    'CC':None, # coordin. conjunction (and, but, or)  \n",
    "    'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "    'DT':None, # determiner (a, the)                    \n",
    "    'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "    'FW':None, # foreign word (mea culpa)             \n",
    "    'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "    'JJ':wn.ADJ, # adjective (yellow)                  \n",
    "    'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "    'JJS':wn.ADJ, # adj., superlative (wildest)           \n",
    "    'LS':None, # list item marker (1, 2, One)          \n",
    "    'MD':None, # modal (can, should)                    \n",
    "    'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "    'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "    'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "    'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "    'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "    'POS':None, # possessive ending (’s )               \n",
    "    'PRP':None, # personal pronoun (I, you, he)     \n",
    "    'PRP$':None, # possessive pronoun (your, one’s)    \n",
    "    'RB':wn.ADV, # adverb (quickly, never)            \n",
    "    'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "    'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "    'RP':wn.ADJ, # particle (up, off)\n",
    "    'SYM':None, # symbol (+,%, &)\n",
    "    'TO':None, # “to” (to)\n",
    "    'UH':None, # interjection (ah, oops)\n",
    "    'VB':wn.VERB, # verb base form (eat)\n",
    "    'VBD':wn.VERB, # verb past tense (ate)\n",
    "    'VBG':wn.VERB, # verb gerund (eating)\n",
    "    'VBN':wn.VERB, # verb past participle (eaten)\n",
    "    'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "    'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "    'WDT':None, # wh-determiner (which, that)\n",
    "    'WP':None, # wh-pronoun (what, who)\n",
    "    'WP$':None, # possessive (wh- whose)\n",
    "    'WRB':None, # wh-adverb (how, where)\n",
    "    '$':None, #  dollar sign ($)\n",
    "    '#':None, # pound sign (#)\n",
    "    '“':None, # left quote (‘ or “)\n",
    "    '”':None, # right quote (’ or ”)\n",
    "    '(':None, # left parenthesis ([, (, {, <)\n",
    "    ')':None, # right parenthesis (], ), }, >)\n",
    "    ',':None, # comma (,)\n",
    "    '.':None, # sentence-final punc (. ! ?)\n",
    "    ':':None # mid-sentence punc (: ; ... – -)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>tag_2</th>\n",
       "      <th>tag_3</th>\n",
       "      <th>tag_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus_1</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>ViralGenome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus_2</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus_3</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus_4</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coronavirus_5</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_name          tag_1         tag_2 tag_3  tag_4\n",
       "0  Coronavirus_1    Coronavirus    ViralGenome   NaN    NaN\n",
       "1  Coronavirus_2     Coronavirus           NaN   NaN    NaN\n",
       "2  Coronavirus_3     Coronavirus           NaN   NaN    NaN\n",
       "3  Coronavirus_4     Coronavirus           NaN   NaN    NaN\n",
       "4  Coronavirus_5     Coronavirus           NaN   NaN    NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"metadata.csv\")\n",
    "metadata = df.copy()\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = list(metadata[\"doc_name\"])\n",
    "doc_names = [doc_name.strip() for doc_name in doc_names]\n",
    "\n",
    "tags1 = list(metadata[\"tag_1\"])\n",
    "tags2 = list(metadata[\"tag_2\"])\n",
    "tags3 = list(metadata[\"tag_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 320236.99it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_tags = []\n",
    "main_doc_tags_unsorted = []\n",
    "for (doc, tag1, tag2, tag3) in tqdm(zip(doc_names, tags1, tags2, tags3)):\n",
    "    main_tag = str(doc).split(\"_\", 1)[0]\n",
    "    main_doc_tags_unsorted.append(main_tag)\n",
    "    \n",
    "    tags = [tag1, tag2, tag3]\n",
    "    tags = [tag.strip() for tag in tags if str(tag) != 'nan']\n",
    "    doc_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tags_dict = dict(zip(doc_names, doc_tags)) \n",
    "main_doc_tags_dict = dict(zip(doc_names, main_doc_tags_unsorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_doc_tags = np.unique(np.array([tag for sublist in doc_tags for tag in sublist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrinkle Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 676.29it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "titles = []\n",
    "abstracts = []\n",
    "main_doc_tags = []\n",
    "\n",
    "corpusDir = \"_corpus/\"\n",
    "files = os.listdir(corpusDir)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    doc = file\n",
    "    docs.append(doc)\n",
    "    main_doc_tags.append(main_doc_tags_dict[file])\n",
    "    \n",
    "    with open(corpusDir + file, \"r\", encoding=\"utf-8\") as f:\n",
    "        title = f.readline().strip()\n",
    "        space = f.readline()\n",
    "        abstract = f.readline().strip()\n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrinkle: splicing together documents (10 random sentences per article)\n",
    "Based on same primary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ethics_60', 'PublicHealthSurveillance_33', 'Ethics_94', 'Telemedicine_73', 'InfectionControl_81']\n",
      "['Ethics', 'PublicHealthSurveillance', 'Ethics', 'Telemedicine', 'InfectionControl']\n"
     ]
    }
   ],
   "source": [
    "print(files[0:5])\n",
    "print(main_doc_tags[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 851.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# abstract sentence tokenize\n",
    "abstract_sentences = []\n",
    "for abstract in tqdm(abstracts):\n",
    "    sent_text = sent_tokenize(abstract)\n",
    "    abstract_sentences.append(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths = [len(abstract) for abstract in abstract_sentences]\n",
    "avg_sentence_len = math.ceil(statistics.mean(abstract_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_by_tag = {tag: [] for tag in unique_doc_tags} \n",
    "for doc, tag, sentences in zip(docs, main_doc_tags, abstract_sentences):\n",
    "    all_sentences_by_tag[tag].extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sentences = {}\n",
    "for tag, sentences in all_sentences_by_tag.items():\n",
    "    num_sentences = len(sentences)\n",
    "    random.shuffle(sentences)\n",
    "    chunks = [sentences[x: x + avg_sentence_len] for x in range(0, num_sentences, avg_sentence_len)]\n",
    "    tag_sentences[tag] = chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new files\n",
    "counter = 0\n",
    "for tag, contents in tag_sentences.items():\n",
    "    for sentences in contents:\n",
    "        doc_name = tag + \"_\" + str(counter).zfill(3)\n",
    "        file = \"_corpus_augmented_full_splice/\" + doc_name\n",
    "        new_text = ' '.join(sentences)\n",
    "        \n",
    "        f = open(file, \"w\")\n",
    "        f.write(new_text)\n",
    "        f.close()\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrinkle: splicing together documents in pairs (50/50 article split)\n",
    "Based on same primary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ethics_60', 'PublicHealthSurveillance_33', 'Ethics_94', 'Telemedicine_73', 'InfectionControl_81']\n",
      "['Ethics_60', 'PublicHealthSurveillance_33', 'Ethics_94', 'Telemedicine_73', 'InfectionControl_81']\n",
      "['Ethics', 'PublicHealthSurveillance', 'Ethics', 'Telemedicine', 'InfectionControl']\n"
     ]
    }
   ],
   "source": [
    "print(docs[0:5])\n",
    "print(files[0:5])\n",
    "print(main_doc_tags[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 287468.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tag_docs = {tag: [] for tag in unique_doc_tags} \n",
    "for file in tqdm(files):\n",
    "    tag = str(file).split(\"_\", 1)[0]\n",
    "    tag_docs[tag].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_combined = 2\n",
    "tag_docs_chunks = {tag: [] for tag in unique_doc_tags} \n",
    "for tag, docs in tag_docs.items():\n",
    "    num_docs = len(docs)\n",
    "    random.shuffle(docs)\n",
    "    chunks = [docs[x: x + num_docs_combined] for x in range(0, num_docs, num_docs_combined)]\n",
    "    tag_docs_chunks[tag].extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc_by_sent(doc):\n",
    "    with open(corpusDir + doc, \"r\", encoding=\"utf-8\") as f:\n",
    "        title = f.readline().strip()\n",
    "        space = f.readline()\n",
    "        abstract = f.readline().strip()\n",
    "        sent_text = sent_tokenize(abstract)\n",
    "        return sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst):\n",
    "    half = len(lst) // 2\n",
    "    return lst[:half], lst[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_random(lst):\n",
    "    random.shuffle(lst)\n",
    "    half = len(lst) // 2\n",
    "    return lst[:half], lst[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = {}\n",
    "new_docs_random = {}\n",
    "for tag, doc_chunks in tag_docs_chunks.items():\n",
    "    for chunk in doc_chunks:\n",
    "        doc1 = chunk[0]\n",
    "        doc2 = chunk[1]\n",
    "        doc1_name = tag + \"_\" + chunk[0].split(\"_\", 1)[1].zfill(3)\n",
    "        doc2_name = tag + \"_\" + chunk[1].split(\"_\", 1)[1].zfill(3)\n",
    "        \n",
    "        sent1 = tokenize_doc_by_sent(doc1)\n",
    "        sent2 = tokenize_doc_by_sent(doc2)\n",
    "\n",
    "        doc11half, doc12half = split_list(sent1)\n",
    "        doc21half, doc22half = split_list(sent2)\n",
    "        \n",
    "        new_doc_name_1 = doc1_name + \"_\" + doc2_name\n",
    "        new_doc_name_2 = doc2_name + \"_\" + doc1_name\n",
    "        \n",
    "        new_doc_sents_1 = doc11half + doc22half\n",
    "        new_doc_sents_2 = doc21half + doc12half\n",
    "        \n",
    "        new_docs[new_doc_name_1] = new_doc_sents_1\n",
    "        new_docs[new_doc_name_2] = new_doc_sents_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new files\n",
    "for doc_name, contents in new_docs.items():\n",
    "    file = \"_corpus_augmented_half_splice_pairs/\" + doc_name\n",
    "    new_text = ' '.join(contents)\n",
    "    f = open(file, \"w\")\n",
    "    f.write(new_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrinkle: splicing together documents randomly (50/50 article split)\n",
    "Based on same primary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halves(first_halves_docs, first_halves, second_halves_docs, second_halves):\n",
    "    first_halves_zip = list(zip(first_halves_docs, first_halves))\n",
    "    random.shuffle(first_halves_zip)\n",
    "    first_halves_docs_random, first_halves_random = zip(*first_halves_zip)\n",
    "\n",
    "    second_halves_zip = list(zip(second_halves_docs, second_halves))\n",
    "    random.shuffle(second_halves_zip)\n",
    "    second_halves_docs_random, second_halves_random = zip(*second_halves_zip)\n",
    "    \n",
    "    return first_halves_docs_random, first_halves_random, second_halves_docs_random, second_halves_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_docs_chunks = {tag: [] for tag in unique_doc_tags} \n",
    "for tag, docs in tag_docs.items():\n",
    "    first_halves_docs = []\n",
    "    first_halves = []\n",
    "    second_halves_docs = []\n",
    "    second_halves = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_name = tag + \"_\" + doc.split(\"_\", 1)[1].zfill(3)\n",
    "        first_halves_docs.append(doc_name)\n",
    "        second_halves_docs.append(doc_name)\n",
    "        \n",
    "        sent = tokenize_doc_by_sent(doc)\n",
    "        dochalf1, dochalf2 = split_list(sent)\n",
    "        first_halves.append(dochalf1)\n",
    "        second_halves.append(dochalf2)\n",
    "    \n",
    "    first_halves_docs_random, first_halves_random, second_halves_docs_random, second_halves_random = get_halves(first_halves_docs, first_halves, second_halves_docs, second_halves)\n",
    "\n",
    "    # create new files\n",
    "    for doc1, half1, doc2, half2 in zip(first_halves_docs_random, first_halves_random, second_halves_docs_random, second_halves_random):\n",
    "        file = \"_corpus_augmented_half_splice/\" + doc1 + \"_\" + doc2\n",
    "        contents = half1 + half2\n",
    "        new_text = ' '.join(contents)\n",
    "\n",
    "        f = open(file, \"w\")\n",
    "        f.write(new_text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1417.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# read in files and get titles / abstracts\n",
    "\n",
    "docs = []\n",
    "titles = []\n",
    "abstracts = []\n",
    "main_doc_tags = []\n",
    "\n",
    "corpusDir = \"_corpus/\"\n",
    "files = os.listdir(corpusDir)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    doc = file\n",
    "    docs.append(doc)\n",
    "    tag = str(file).split(\"_\", 1)[0]\n",
    "    main_doc_tags.append(tag)\n",
    "    \n",
    "    with open(corpusDir + file, \"r\", encoding=\"utf-8\") as f:     \n",
    "        title = f.readline().strip()\n",
    "        space = f.readline()\n",
    "        abstract = f.readline().strip()\n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and handle nan abstracts\n",
    "\n",
    "all_titles = [str(title).lower().translate(str.maketrans('', '', string.punctuation)) for title in titles]\n",
    "all_abstracts = [str(abstract).lower().translate(str.maketrans('', '', string.punctuation)) \n",
    "                 for abstract in abstracts]\n",
    "\n",
    "all_abstracts = ['' if abstract == 'nan' else abstract for abstract in all_abstracts]\n",
    "\n",
    "assert len(all_titles) == len(all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 853.99it/s] \n"
     ]
    }
   ],
   "source": [
    "# tokenize titles\n",
    "\n",
    "tokenized_titles = []\n",
    "tokenized_title_tags = []\n",
    "for title in tqdm(all_titles):\n",
    "    words = word_tokenize(title)\n",
    "    tuples = pos_tag(words)\n",
    "    tokens = [tup[0] for tup in tuples if tup[0] not in swlist and len(tup[0]) > 2 and tup[0].isalpha()]\n",
    "    tags = [tag_map[tup[1]] for tup in tuples if tup[0] not in swlist and len(tup[0]) > 2 and tup[0].isalpha()]\n",
    "    assert len(tokens) == len(tags)\n",
    "    \n",
    "    tokenized_titles.append(tokens)\n",
    "    tokenized_title_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:16<00:00, 123.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize abstracts\n",
    "\n",
    "tokenized_abstracts = []\n",
    "tokenized_abstract_tags = []\n",
    "for abstract in tqdm(all_abstracts):\n",
    "    words = word_tokenize(abstract)\n",
    "    tuples = pos_tag(words)\n",
    "    tokens = [tup[0] for tup in tuples if tup[0] not in swlist and len(tup[0]) > 2 and tup[0].isalpha()]\n",
    "    tags = [tag_map[tup[1]] for tup in tuples if tup[0] not in swlist and len(tup[0]) > 2 and tup[0].isalpha()]\n",
    "        \n",
    "    tokenized_abstracts.append(tokens)\n",
    "    tokenized_abstract_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 253646.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# intermediate texts (title + abstract before lemmatization)\n",
    "\n",
    "intermediate_texts = []\n",
    "for (title, abstract) in tqdm(zip(tokenized_titles, tokenized_abstracts)):\n",
    "    intermediate_texts.append(title + abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 24563.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# lemmatize titles\n",
    "\n",
    "all_lemm_titles = []\n",
    "for (tokens, tags) in tqdm(zip(tokenized_titles, tokenized_title_tags)):\n",
    "    lemm_tokens = [wn.lemmatize(token, tag) for (token, tag) in zip(tokens, tags) if tag is not None]\n",
    "    all_lemm_titles.append(lemm_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:01, 1141.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# lemmatize abstracts\n",
    "\n",
    "all_lemm_abstracts = []\n",
    "for (tokens, tags) in tqdm(zip(tokenized_abstracts, tokenized_abstract_tags)):\n",
    "    lemm_tokens = [wn.lemmatize(token, tag) for (token, tag) in zip(tokens, tags) if tag is not None]\n",
    "    all_lemm_abstracts.append(lemm_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles and abstracts post-processing\n",
    "\n",
    "final_titles = all_lemm_titles\n",
    "final_abstracts = all_lemm_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 290102.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# final texts (title + abstract after lemmatization)\n",
    "\n",
    "final_texts = []\n",
    "for (title, abstract) in tqdm(zip(final_titles, final_abstracts)):\n",
    "    final_texts.append(title + abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find padding / truncation boundary such that most text content is preserved\n",
    "\n",
    "text_lens = [len(text) for text in final_texts]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(text_lens)\n",
    "plt.title(\"Distribution of Document Word Counts\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification: Simple RNN (One-to-One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model (this may take some time)...\n",
      "Word2Vec model loaded.\n",
      "CPU times: user 2min 51s, sys: 2.11 s, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Loading Word2Vec model (this may take some time)...\")\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "print(\"Word2Vec model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18798 unique tokens. Distilled to 18798 top words.\n",
      "Shape of data tensor: (2000, 400)\n",
      "Shape of label tensor: (2000, 19)\n",
      "18798\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = 400 # maximum and minimum number of words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(final_texts)\n",
    "sequences = tokenizer.texts_to_sequences(final_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "# convert all labels to factors and then categorical (int) data\n",
    "\n",
    "(labelFactors, factorLabels) = pd.factorize(main_doc_tags)\n",
    "y_ohe = keras.utils.to_categorical(labelFactors.astype(np.uint8))\n",
    "\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Coronavirus', 'Ethics', 'Immunity', 'InfectionControl',\n",
       "       'InfectiousDiseaseTransmission', 'InformationDissemination',\n",
       "       'Livestock', 'PointOfCareTesting', 'PostExposureProphylaxis',\n",
       "       'ProtectiveFactors', 'PublicHealthSurveillance', 'RiskFactors',\n",
       "       'SkilledNursingFacility', 'SocialChange', 'SocialStigma',\n",
       "       'Telemedicine', 'ViralGenome', 'ViralVaccines',\n",
       "       'VulnerablePopulations'], dtype='<U29')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(main_doc_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 400) (1600, 19)\n",
      "[ 87.  77.  84.  82.  79.  83.  74.  83. 161.  83.  81.  79.  80.  76.\n",
      "  71.  81.  82.  82.  75.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2, random_state=42)\n",
    "\n",
    "NUM_CLASSES = 19\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18799, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 300 # from google news embeddings\n",
    "\n",
    "# now fill in the matrix, using the ordering from the keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_vec = word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vec\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 300)          5639700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19)                1919      \n",
      "=================================================================\n",
      "Total params: 5,802,019\n",
      "Trainable params: 162,319\n",
      "Non-trainable params: 5,639,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "1600/1600 [==============================] - 31s 19ms/step - loss: 2.7148 - accuracy: 0.1550 - val_loss: 2.4309 - val_accuracy: 0.1900\n",
      "Epoch 2/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 2.4192 - accuracy: 0.2688 - val_loss: 2.1423 - val_accuracy: 0.3275\n",
      "Epoch 3/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 2.1922 - accuracy: 0.3494 - val_loss: 2.0180 - val_accuracy: 0.3675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb5014b2f60>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.9353 - accuracy: 0.4487 - val_loss: 1.6773 - val_accuracy: 0.5125\n",
      "Epoch 2/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.7479 - accuracy: 0.4806 - val_loss: 1.5112 - val_accuracy: 0.5275\n",
      "Epoch 3/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.5985 - accuracy: 0.5375 - val_loss: 1.4049 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb50148b518>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.4716 - accuracy: 0.5856 - val_loss: 1.2849 - val_accuracy: 0.6100\n",
      "Epoch 2/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.3650 - accuracy: 0.6094 - val_loss: 1.2461 - val_accuracy: 0.6075\n",
      "Epoch 3/3\n",
      "1600/1600 [==============================] - 30s 19ms/step - loss: 1.2494 - accuracy: 0.6356 - val_loss: 1.2075 - val_accuracy: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb50148b748>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification: Simple CNN (One-to-One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 300)          5639700   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 396, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 11, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 6,031,271\n",
      "Trainable params: 391,571\n",
      "Non-trainable params: 5,639,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_SIZE = 300 \n",
    "sequence_input = Input(shape=(MAX_ART_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(5)(x)  \n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.8581 - acc: 0.1356 - val_loss: 2.6022 - val_acc: 0.2225\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.4884 - acc: 0.2338 - val_loss: 2.3080 - val_acc: 0.2675\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.1418 - acc: 0.3519 - val_loss: 2.0145 - val_acc: 0.3625\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 1.7227 - acc: 0.4994 - val_loss: 1.8522 - val_acc: 0.4100\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 1.4346 - acc: 0.5681 - val_loss: 1.5335 - val_acc: 0.5350\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 1.0811 - acc: 0.6913 - val_loss: 1.4967 - val_acc: 0.5750\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 1.0515 - acc: 0.6969 - val_loss: 1.2168 - val_acc: 0.6525\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.8543 - acc: 0.7575 - val_loss: 1.2624 - val_acc: 0.6300\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.7859 - acc: 0.7900 - val_loss: 1.0526 - val_acc: 0.7150\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5803 - acc: 0.8512 - val_loss: 2.1829 - val_acc: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4d8504f98>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.7860 - acc: 0.7837 - val_loss: 0.9964 - val_acc: 0.7225\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5400 - acc: 0.8581 - val_loss: 1.7471 - val_acc: 0.5550\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5319 - acc: 0.8544 - val_loss: 1.0182 - val_acc: 0.7375\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6986 - acc: 0.8131 - val_loss: 0.9856 - val_acc: 0.7175\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4511 - acc: 0.8825 - val_loss: 1.0093 - val_acc: 0.7375\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.5343 - acc: 0.8600 - val_loss: 2.1965 - val_acc: 0.5150\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.5455 - acc: 0.8481 - val_loss: 0.9726 - val_acc: 0.7425\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.4203 - acc: 0.8850 - val_loss: 0.9807 - val_acc: 0.7375\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.7123 - acc: 0.8238 - val_loss: 1.2962 - val_acc: 0.6675\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4443 - acc: 0.8788 - val_loss: 1.0500 - val_acc: 0.7225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb650660048>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4153 - acc: 0.8863 - val_loss: 0.9966 - val_acc: 0.7475\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4068 - acc: 0.8869 - val_loss: 1.0794 - val_acc: 0.7375\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.7677 - acc: 0.8275 - val_loss: 0.9859 - val_acc: 0.7450\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4062 - acc: 0.8863 - val_loss: 0.9953 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.4073 - acc: 0.8863 - val_loss: 1.0035 - val_acc: 0.7475\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.4033 - acc: 0.8875 - val_loss: 1.0296 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.6806 - acc: 0.8338 - val_loss: 0.9569 - val_acc: 0.7375\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.4074 - acc: 0.8863 - val_loss: 0.9523 - val_acc: 0.7600\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.4009 - acc: 0.8875 - val_loss: 0.9829 - val_acc: 0.7575\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.3989 - acc: 0.8875 - val_loss: 1.0079 - val_acc: 0.7525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4d8449048>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification: Larger CNN (One-to-One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 300)          5639700   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 396, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 11, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 6,031,271\n",
      "Trainable params: 391,571\n",
      "Non-trainable params: 5,639,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 300 \n",
    "sequence_input = Input(shape=(MAX_ART_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(3)(x)  \n",
    "x = Conv1D(128, 5, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(3)(x)  \n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_med = Model(sequence_input, preds)\n",
    "model_med.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.9004 - acc: 0.0931 - val_loss: 2.8759 - val_acc: 0.1375\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.6526 - acc: 0.1825 - val_loss: 2.6708 - val_acc: 0.1775\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.4227 - acc: 0.2587 - val_loss: 2.2700 - val_acc: 0.2950\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 2.1307 - acc: 0.3494 - val_loss: 2.1353 - val_acc: 0.3275\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 1.8412 - acc: 0.4412 - val_loss: 2.5807 - val_acc: 0.2700\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 1.7098 - acc: 0.4894 - val_loss: 1.8463 - val_acc: 0.4475\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4909 - acc: 0.5412 - val_loss: 1.6453 - val_acc: 0.5075\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3362 - acc: 0.6112 - val_loss: 1.5569 - val_acc: 0.5125\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1992 - acc: 0.6519 - val_loss: 1.5250 - val_acc: 0.5425\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1075 - acc: 0.6787 - val_loss: 1.6488 - val_acc: 0.5100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4d82caf28>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_med.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.9057 - acc: 0.7494 - val_loss: 1.6064 - val_acc: 0.5275\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.9640 - acc: 0.7169 - val_loss: 1.6688 - val_acc: 0.5300\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.9418 - acc: 0.7550 - val_loss: 1.5989 - val_acc: 0.5375\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.7455 - acc: 0.7925 - val_loss: 1.7990 - val_acc: 0.5350\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.8374 - acc: 0.7544 - val_loss: 2.8697 - val_acc: 0.3825\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.9061 - acc: 0.7513 - val_loss: 1.2711 - val_acc: 0.6575\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6636 - acc: 0.8150 - val_loss: 1.9400 - val_acc: 0.5400\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.8545 - acc: 0.7625 - val_loss: 1.3107 - val_acc: 0.6450\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6431 - acc: 0.8175 - val_loss: 1.3255 - val_acc: 0.6550\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.0042 - acc: 0.7400 - val_loss: 1.5980 - val_acc: 0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb502241cc0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_med.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6642 - acc: 0.8150 - val_loss: 1.3020 - val_acc: 0.6425\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6232 - acc: 0.8238 - val_loss: 1.3988 - val_acc: 0.6400\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6131 - acc: 0.8238 - val_loss: 1.4022 - val_acc: 0.6500\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6022 - acc: 0.8244 - val_loss: 1.6037 - val_acc: 0.6375\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2139 - acc: 0.7519 - val_loss: 1.3642 - val_acc: 0.6575\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5997 - acc: 0.8263 - val_loss: 1.4192 - val_acc: 0.6325\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5969 - acc: 0.8269 - val_loss: 1.4301 - val_acc: 0.6575\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5949 - acc: 0.8275 - val_loss: 1.5001 - val_acc: 0.6525\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.0382 - acc: 0.7556 - val_loss: 1.3480 - val_acc: 0.6475\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5961 - acc: 0.8269 - val_loss: 1.3752 - val_acc: 0.6525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4d8176518>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_med.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
