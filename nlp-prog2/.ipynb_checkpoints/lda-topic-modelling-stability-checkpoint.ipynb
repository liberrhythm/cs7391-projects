{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "from nltk import FreqDist, sent_tokenize, word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.data import load\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_Model:\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, directory, output, params):\n",
    "        # set class member variables\n",
    "        self.directory = directory + '/'\n",
    "        self.output = output\n",
    "        self.params = self.__get_params(params)\n",
    "\n",
    "        # initialize helpful variables\n",
    "        self.docs = self.__get_documents(directory)\n",
    "        self.num_docs = len(self.docs)\n",
    "        self.swlist = self.__get_stopwords(self.params[6])\n",
    "        \n",
    "        # get tokens and sentences from corpus of documents\n",
    "        self.all_tokens = []\n",
    "        self.all_sentences = self.__read_files()\n",
    "        self.__filter_by_pos(self.params[4])\n",
    "        self.__remove_stopwords(1)\n",
    "        self.__filter_out_tokens(self.params[3])\n",
    "        self.__lemmatize_or_stem(self.params[5])\n",
    "        self.__remove_stopwords(2)\n",
    "                \n",
    "        # get vector model\n",
    "        self.__get_vector_model(self.params[1])\n",
    "        \n",
    "    # private helper methods\n",
    "    \n",
    "    # read in and process parameters text file\n",
    "    def __get_params(self, params_file_name):\n",
    "        defaults = [8, 'B', \"auto\", 'n', 'A', 'L', 'nltk']\n",
    "        \n",
    "        if params_file_name == \"\":\n",
    "            return defaults\n",
    "        \n",
    "        # read parameters file\n",
    "        params_file = open(params_file_name, 'r') \n",
    "        params_lines = params_file.readlines() \n",
    "\n",
    "        params = []\n",
    "\n",
    "        # get all parameters from file\n",
    "        for param_line in params_lines: \n",
    "            param_vals = param_line.strip().split()\n",
    "            if len(param_vals) == 0:\n",
    "                params.append(\"\")\n",
    "            else:\n",
    "                params.append(param_vals[0])\n",
    "        \n",
    "        # go through all params and set to default if needed\n",
    "        for idx, param in enumerate(params):\n",
    "            if param == \"\":\n",
    "                params[idx] = defaults[idx]\n",
    "                \n",
    "        return params\n",
    "\n",
    "    \n",
    "    # get list of documents\n",
    "    def __get_documents(self, path):\n",
    "        return os.listdir(path)    \n",
    "\n",
    "    \n",
    "    # returns set of stopwords (empty, nltk default, or through text file)\n",
    "    def __get_stopwords(self, stopword):\n",
    "        regex = re.compile('[^a-z]')\n",
    "\n",
    "        # returns set of stopwords (empty, nltk default, or through text file)\n",
    "        if (stopword == \"none\"):\n",
    "            # return empty set if no stopwords provided\n",
    "            swlist = set()\n",
    "        elif (stopword == \"nltk\"):\n",
    "            # return default nltk set of english stopwords\n",
    "            swlist = set(stopwords.words(\"english\"))\n",
    "        else:\n",
    "            # populate stopword set from text file\n",
    "            with open(stopword) as f:\n",
    "                stopword_list = [regex.sub('', word) for line in f for word in re.split('[;,.\\-\\n ]', line) if word]\n",
    "            f.close()\n",
    "            swlist = set(stopword_list)\n",
    "\n",
    "        return swlist\n",
    "\n",
    "        # removes stopwords from list of tokens\n",
    "    def __remove_stopwords(self, param):\n",
    "        if param == 1:\n",
    "            all_sentences = []\n",
    "            for doc in self.all_sentences:\n",
    "                new_doc = [[token for token in sent if token not in self.swlist] for sent in doc]\n",
    "                all_sentences.append(new_doc)\n",
    "            self.all_sentences = all_sentences\n",
    "        elif param == 2:\n",
    "            self.all_tokens = [[token for token in tokens if token not in self.swlist] for tokens in self.all_tokens]\n",
    "    \n",
    "    # reads in text from files\n",
    "    def __read_files(self):\n",
    "        all_sentences = []\n",
    "        \n",
    "        for file in self.docs:\n",
    "            inFile = open(os.path.join(self.directory + file), 'r')\n",
    "            text = inFile.read()\n",
    "            sentences = sent_tokenize(text.lower())\n",
    "            all_sentences.append(sentences)\n",
    "        \n",
    "        tokenized_sentences = [[word_tokenize(sent) for sent in sentences] for sentences in all_sentences]\n",
    "        return tokenized_sentences\n",
    "    \n",
    "    # filter by part of speech\n",
    "    def __filter_by_pos(self, param):\n",
    "        tag_dict = load('help/tagsets/upenn_tagset.pickle')\n",
    "        all_tags = list(tag_dict.keys())\n",
    "        tokenized_sentences = self.all_sentences\n",
    "        tagged_words = [pos_tag_sents(sentences) for sentences in tokenized_sentences]\n",
    "        \n",
    "        adjective = ['JJ', 'JJR', 'JJS']\n",
    "        verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "        noun = ['NN', 'NNP', 'NNPS', 'NNS',]\n",
    "        adverb = ['RB', 'RBR', 'RBS']\n",
    "\n",
    "        if param == 'F':\n",
    "            allowed_tag_list = noun + verb + adjective + adverb\n",
    "        elif param == 'N':\n",
    "            allowed_tag_list = noun + adjective\n",
    "        elif param == 'n':\n",
    "            allowed_tag_list = noun\n",
    "        else:\n",
    "            allowed_tag_list = all_tags\n",
    "        \n",
    "        all_new_tokens = []\n",
    "        all_sent_dicts = []\n",
    "        \n",
    "        for doc in tagged_words:\n",
    "            doc_words = []\n",
    "            doc_dicts = []\n",
    "            \n",
    "            for sentence in doc:\n",
    "                new_tokens = [word for word, tag in sentence if tag in allowed_tag_list] # and word not in self.swlist]\n",
    "                doc_words.append(new_tokens)\n",
    "                sent_dict = dict(sentence)\n",
    "                doc_dicts.append(sent_dict)\n",
    "                \n",
    "            all_new_tokens.append(doc_words)\n",
    "            all_sent_dicts.append(doc_dicts)\n",
    "        \n",
    "        self.all_tokens = all_new_tokens # currently in sentence form\n",
    "        self.all_sent_dicts = all_sent_dicts\n",
    "        \n",
    "    # new preprocessing function\n",
    "    def __filter_out_tokens(self, param):\n",
    "        delchars = ''.join(c for c in map(chr, range(256)) if not c.isalnum())\n",
    "        all_tokens = []\n",
    "        \n",
    "        for doc in self.all_tokens:\n",
    "            doc_tokens = []\n",
    "            for sentence in doc:\n",
    "                # tokens = re.split(\"[, \\-!?:;.]+\", text.lower())\n",
    "\n",
    "                if param == 'A':\n",
    "                    tokens = [token for token in sentence if token.isalnum() or len(token) > 1]\n",
    "                elif param == 'a':\n",
    "                    tokens = [token.translate(str.maketrans('', '', delchars)) for token in sentence if token.isalnum() or len(token) > 1]\n",
    "                elif param == 'N':\n",
    "                    tokens = [token for token in sentence if token.isalnum()]\n",
    "                elif param == 'n':\n",
    "                    tokens = [token for token in sentence if token.isalnum() and not token.isdigit()]\n",
    "\n",
    "                doc_tokens.append(tokens)\n",
    "            all_tokens.append(doc_tokens)\n",
    "\n",
    "        self.all_tokens = all_tokens\n",
    "        \n",
    "    \n",
    "    # lemmatize or stem tokens\n",
    "    def __lemmatize_or_stem(self, param):\n",
    "        if param == 'N':\n",
    "            all_tokens = []\n",
    "            for doc in self.all_tokens:\n",
    "                flat_list = [token for sentence in doc for token in sentence]\n",
    "                all_tokens.append(flat_list)\n",
    "            new_tokens = all_tokens\n",
    "        elif param == 'B':\n",
    "            stemmer = PorterStemmer()\n",
    "            all_tokens = []\n",
    "            for doc in self.all_tokens:\n",
    "                flat_list = [stemmer.stem(token) for sentence in doc for token in sentence]\n",
    "                all_tokens.append(flat_list)\n",
    "                \n",
    "            # stemmed = [[stemmer.stem(token) for token in tokens] for tokens in all_tokens]\n",
    "            new_tokens = all_tokens\n",
    "        elif param == 'L':\n",
    "            tag_map = {\n",
    "                'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "                'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "                'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "                'JJ':wn.ADJ, # adjective (yellow)                  \n",
    "                'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "                'JJS':wn.ADJ, # adj., superlative (wildest)                             \n",
    "                'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "                'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "                'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "                'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "                'PDT':wn.ADJ, # predeterminer (all, both)             \n",
    "                'RB':wn.ADV, # adverb (quickly, never)            \n",
    "                'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "                'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "                'RP':wn.ADJ, # particle (up, off)\n",
    "                'VB':wn.VERB, # verb base form (eat)\n",
    "                'VBD':wn.VERB, # verb past tense (ate)\n",
    "                'VBG':wn.VERB, # verb gerund (eating)\n",
    "                'VBN':wn.VERB, # verb past participle (eaten)\n",
    "                'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "                'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "            }\n",
    "            \n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            lemmatized_tokens = []\n",
    "            for idx1, doc in enumerate(self.all_tokens):\n",
    "                lemmatized_doc = []\n",
    "                for idx2, sentence in enumerate(doc):\n",
    "                    for idx3, token in enumerate(sentence):\n",
    "                        pos = self.all_sent_dicts[idx1][idx2][token]\n",
    "                        if pos not in tag_map:\n",
    "                            lemmatized = lemmatizer.lemmatize(token)\n",
    "                        else:\n",
    "                            wn_pos = tag_map[pos]\n",
    "                            lemmatized = lemmatizer.lemmatize(token, wn_pos)\n",
    "                        \n",
    "                        lemmatized_doc.append(lemmatized)\n",
    "                lemmatized_tokens.append(lemmatized_doc)\n",
    "            new_tokens = lemmatized_tokens\n",
    "        \n",
    "        self.all_tokens = new_tokens # now in document form\n",
    "\n",
    "    def __get_vector_model(self, param):\n",
    "        # alter and return term frequency matrix based on given tf parameter\n",
    "        if param == 'T':\n",
    "            smartirs = 'nfn'\n",
    "        elif param == 'B':\n",
    "            smartirs = 'bnn'\n",
    "        elif param == 't':\n",
    "            smartirs = 'nnn'\n",
    "        \n",
    "        self.dictionary = Dictionary(self.all_tokens)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.all_tokens]\n",
    "        model = TfidfModel(corpus=self.corpus, id2word=self.dictionary, smartirs=smartirs)        \n",
    "        vector_model = []\n",
    "        for idx, doc in enumerate(self.docs):\n",
    "            vector_model.append(model[self.corpus[idx]])\n",
    "        \n",
    "        self.vector_model = vector_model\n",
    "    \n",
    "    # remove stopwords \n",
    "    def generate_model(self):\n",
    "        # set training parameters.\n",
    "        num_topics = self.params[0]\n",
    "        alpha = self.params[2]\n",
    "        corpus = self.vector_model\n",
    "        dictionary = self.dictionary\n",
    "\n",
    "        # create model\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            alpha=alpha,\n",
    "            eta='auto',\n",
    "            num_topics=num_topics\n",
    "        )\n",
    "        \n",
    "        return [corpus, model]\n",
    "        \n",
    "    # save model to file(s)\n",
    "    def save_model(self, model, corpus):\n",
    "        # save entire model\n",
    "        model.save(self.output + \".model\")\n",
    "        \n",
    "        # save topic-word matrix\n",
    "        total_topics = len(model.get_topics())\n",
    "        \n",
    "        for idx in range(0, total_topics):\n",
    "            outFile = open(self.output + '_' + str(idx) + \".topic\", \"w\")\n",
    "            topic_terms = model.show_topic(idx, topn=10000000)\n",
    "            for topic_term in topic_terms:\n",
    "                outFile.write(topic_term[0] + ' ' + str(topic_term[1]) + '\\n')\n",
    "            outFile.close()\n",
    "            \n",
    "        # save document-topic matrix\n",
    "        outFile = open(self.output + \".dt\", \"w\")\n",
    "\n",
    "        for idx in range(0, len(self.docs)):\n",
    "            doc_topics = model.get_document_topics(corpus[idx])\n",
    "            dictionary = dict(doc_topics)\n",
    "            doc_probs = []\n",
    "            for i in range(0, total_topics):\n",
    "                if i in dictionary:\n",
    "                    prob = dictionary[i]\n",
    "                    doc_probs.append(prob)\n",
    "                else:\n",
    "                    doc_probs.append(0.0)\n",
    "            line = self.docs[idx] + ' ' + ' '.join(str(doc_prob) for doc_prob in doc_probs) + '\\n'\n",
    "            outFile.write(line)\n",
    "\n",
    "        outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"_test0\"\n",
    "output = \"output1\"\n",
    "params = \"params.txt\"\n",
    "ldamodel = LDA_Model(directory, output, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, model = ldamodel.generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save_model(model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topic_sets(t_prime, u_prime):\n",
    "    # do topic assignments between sets\n",
    "    max_similarities = []\n",
    "    for idx1, T in enumerate(t_prime):\n",
    "        max_similarity = (None, 0)\n",
    "        for idx2, U in enumerate(u_prime):\n",
    "            t = set([token for token, prob in T])\n",
    "            u = set([token for token, prob in U])\n",
    "            \n",
    "            # calculate jaccard coefficient between topics and store highest\n",
    "            jaccard = len(t & u) / len(t | u)\n",
    "            if jaccard > max_similarity[1]:\n",
    "                max_similarity = (idx2, jaccard)\n",
    "\n",
    "        max_similarities.append(max_similarity)\n",
    "        \n",
    "    ms = np.array(max_similarities)\n",
    "    num_topics = np.shape(ms)[0]\n",
    "    print(\"Original Topic Assignment\")\n",
    "    for idx in range(0, num_topics):\n",
    "        print(str(idx) + \": \" + str(int(ms[idx][0])) + \", \" + str(ms[idx][1]))\n",
    "    print()\n",
    "    \n",
    "    # take care of imperfect matches\n",
    "    u_vals = np.unique(ms[:,0])\n",
    "    for val in u_vals:\n",
    "        same_topic = ms[ms[:,0] == val]\n",
    "        if len(same_topic) > 1:\n",
    "            max_sim = np.amax(same_topic[:,1])\n",
    "            ms[(ms[:,0] == val) & (ms[:,1] != max_sim),1] = 0\n",
    "    \n",
    "    print(\"New Topic Assignment\")\n",
    "    for idx in range(0, num_topics):\n",
    "        print(str(idx) + \": \" + str(int(ms[idx][0])) + \", \" + str(ms[idx][1]))\n",
    "    print()\n",
    "    \n",
    "    topics_used = len(u_vals) / len(u_prime)\n",
    "    similarity_sum = np.sum(ms[:,1])\n",
    "    print(\"Topics Used: \" + str(topics_used))\n",
    "    print(\"Similarity Sum: \" + str(similarity_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1\n",
    "directory = \"_corpus2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_11 = LDA_Model(directory, \"output_11\", \"params_11.txt\")\n",
    "ldamodel_12 = LDA_Model(directory, \"output_12\", \"params_12.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_11, model_11 = ldamodel_11.generate_model()\n",
    "corpus_12, model_12 = ldamodel_12.generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic_terms_11 = []\n",
    "for idx in range(0, int(ldamodel_11.params[0])):\n",
    "    topic_terms = model_11.show_topic(idx, topn=k)\n",
    "    new_topic_terms_11.append(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swlist = set(stopwords.words(\"english\"))\n",
    "\n",
    "new_topic_terms_12 = []\n",
    "for idx in range(0, int(ldamodel_12.params[0])):\n",
    "    topic_terms = model_12.show_topic(idx, topn=10000000)\n",
    "    new_topic_terms = [(token, prob) for token, prob in topic_terms if token not in swlist]\n",
    "    new_topic_terms_12.append(new_topic_terms[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_11, new_topic_terms_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_12, new_topic_terms_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_21 = LDA_Model(directory, \"output_21\", \"params_21.txt\")\n",
    "ldamodel_22 = LDA_Model(directory, \"output_22\", \"params_22.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_21, model_21 = ldamodel_21.generate_model()\n",
    "corpus_22, model_22 = ldamodel_22.generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic_terms_21 = []\n",
    "for idx in range(0, int(ldamodel_21.params[0])):\n",
    "    topic_terms = model_21.show_topic(idx, topn=k)\n",
    "    new_topic_terms_21.append(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective = ['JJ', 'JJR', 'JJS']\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "noun = ['NN', 'NNP', 'NNPS', 'NNS',]\n",
    "adverb = ['RB', 'RBR', 'RBS']\n",
    "\n",
    "allowed_tag_list = noun + verb + adjective + adverb\n",
    "\n",
    "new_topic_terms_22 = []\n",
    "for idx in range(0, int(ldamodel_22.params[0])):\n",
    "    topic_terms = model_22.show_topic(idx, topn=10000000)\n",
    "    tokens = [token for token, prob in topic_terms]\n",
    "    pos_vals = pos_tag(tokens)\n",
    "    \n",
    "    new_topic_terms = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if pos_vals[idx][1] in allowed_tag_list:\n",
    "            new_topic_terms.append((token, topic_terms[idx][1]))\n",
    "    new_topic_terms_22.append(new_topic_terms[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_21, new_topic_terms_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_22, new_topic_terms_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_31 = LDA_Model(directory, \"output_31\", \"params_31.txt\")\n",
    "ldamodel_32 = LDA_Model(directory, \"output_32\", \"params_32.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_31, model_31 = ldamodel_31.generate_model()\n",
    "corpus_32, model_32 = ldamodel_32.generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic_terms_31 = []\n",
    "for idx in range(0, int(ldamodel_31.params[0])):\n",
    "    topic_terms = model_31.show_topic(idx, topn=k)\n",
    "    new_topic_terms_31.append(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = ['NN', 'NNP', 'NNPS', 'NNS',]\n",
    "\n",
    "allowed_tag_list = noun\n",
    "\n",
    "new_topic_terms_32 = []\n",
    "for idx in range(0, int(ldamodel_32.params[0])):\n",
    "    topic_terms = model_32.show_topic(idx, topn=10000000)\n",
    "    tokens = [token for token, prob in topic_terms]\n",
    "    pos_vals = pos_tag(tokens)\n",
    "    \n",
    "    new_topic_terms = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if pos_vals[idx][1] in allowed_tag_list:\n",
    "            new_topic_terms.append((token, topic_terms[idx][1]))\n",
    "    new_topic_terms_32.append(new_topic_terms[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_31, new_topic_terms_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_sets(new_topic_terms_32, new_topic_terms_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
